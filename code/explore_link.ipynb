{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This program will try to explore all the link of a specific type",
   "id": "16d3d2a1c8aa4930"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All imported libraries",
   "id": "940c7e3eafa5cd88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T22:59:58.619599Z",
     "start_time": "2025-05-17T22:59:58.616157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib.parse\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Define 1GB in bytes\n",
    "ONE_GB = 1_073_741_824\n",
    "HUNDRED_MB = 1000000 * 100\n",
    "# Define 100GB in bytes\n",
    "MAX_TOTAL_SIZE = 20 * ONE_GB\n",
    "\n",
    "link_to_explore = \"https://www.uni-bamberg.de/en/its/\""
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:00:00.497924Z",
     "start_time": "2025-05-17T23:00:00.493946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_visited(file_path):\n",
    "    \"\"\"Load visited URLs from a file into a set.\"\"\"\n",
    "    visited = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                url = line.strip()\n",
    "                if url:\n",
    "                    visited.add(url)\n",
    "    return visited\n",
    "\n"
   ],
   "id": "def3125a9a341e8e",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:00:02.102986Z",
     "start_time": "2025-05-17T23:00:02.099728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_directories():\n",
    "    os.makedirs(\"input/explore\", exist_ok=True)\n",
    "    os.makedirs(\"output/explore\", exist_ok=True)\n"
   ],
   "id": "20220a3617b3acc2",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:00:03.534986Z",
     "start_time": "2025-05-17T23:00:03.531739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_visited(file_path, visited):\n",
    "    \"\"\"Save visited URLs to a file (one URL per line).\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in sorted(visited):\n",
    "            f.write(url + \"\\n\")\n"
   ],
   "id": "daf56a10b6e6bbc3",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:00:05.186058Z",
     "start_time": "2025-05-17T23:00:05.182057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pending(file_path):\n",
    "    \"\"\"Load pending URLs from a file into a deque.\"\"\"\n",
    "    pending = deque()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                url = line.strip()\n",
    "                if url:\n",
    "                    pending.append(url)\n",
    "    return pending\n",
    "\n"
   ],
   "id": "627b454a473621e7",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:00:07.744465Z",
     "start_time": "2025-05-17T23:00:07.740846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_pending(file_path, pending):\n",
    "    \"\"\"Save pending URLs to a file (one URL per line).\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in pending:\n",
    "            f.write(url + \"\\n\")\n",
    "\n"
   ],
   "id": "f2973a1b612b669c",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:00:14.104903Z",
     "start_time": "2025-05-17T23:00:14.100308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flush_data(data, batch_index):\n",
    "    \"\"\"Flush the data into a JSON file and return the filename.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_filename = f\"output/scraped_data_{timestamp}_batch{batch_index}.json\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(data, outfile, ensure_ascii=False, indent=2)\n",
    "    print(f\"Flushed {len(data)} records to {output_filename}\")\n",
    "    return output_filename\n"
   ],
   "id": "61193df9bf85ec50",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:21:59.106008Z",
     "start_time": "2025-05-17T23:21:59.096865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scrape_website(base_url, max_pages=0, visited_file=\"input/explore/visited_urls.txt\", pending_file=\"input/explore/pending_urls.txt\"):\n",
    "    \"\"\"\n",
    "    Scrape the website starting at base_url.\n",
    "\n",
    "    If max_pages is set to 0, the scraper runs until no more pending links remain.\n",
    "    Otherwise, it stops after scraping max_pages pages.\n",
    "\n",
    "    Now stops when total scraped data size reaches 100GB.\n",
    "    \"\"\"\n",
    "    # Load previously visited URLs and pending URLs.\n",
    "    visited = load_visited(visited_file)\n",
    "    to_visit = load_pending(pending_file)\n",
    "\n",
    "    # If base_url is not visited and not in the pending list, add it.\n",
    "    if base_url not in visited and base_url not in to_visit:\n",
    "        to_visit.append(base_url)\n",
    "\n",
    "    data = []  # List to store the scraped data.\n",
    "    batch_index = 1  # Batch counter for JSON flushing.\n",
    "    count = 0  # Counter for the number of scraped pages.\n",
    "    total_scraped_size = 0  # Total size of scraped data in bytes.\n",
    "\n",
    "    # Continue scraping while there are URLs to visit, the max_pages condition holds,\n",
    "    # and the total scraped data size is below MAX_TOTAL_SIZE.\n",
    "    while to_visit and (max_pages == 0 or count < max_pages) and total_scraped_size < MAX_TOTAL_SIZE:\n",
    "        url = to_visit.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Scraping: {count} {url}\")\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Skipping {url} due to response status: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            # Queue the internal links.\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                href = link.get(\"href\")\n",
    "                if href is None or href.startswith(\"#\"):\n",
    "                    continue\n",
    "                full_url = urllib.parse.urljoin(url, href)\n",
    "                if full_url.startswith(base_url) and full_url not in visited and full_url not in to_visit:\n",
    "                    to_visit.append(full_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Flush any remaining data even if it hasn't reached 1GB.\n",
    "    if data:\n",
    "        flush_data(data, batch_index)\n",
    "\n",
    "    # Save the updated visited URLs and pending URLs.\n",
    "    save_visited(visited_file, visited)\n",
    "    save_pending(pending_file, to_visit)\n",
    "    return count\n"
   ],
   "id": "579749ea3f8782f1",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:22:19.961407Z",
     "start_time": "2025-05-17T23:22:12.220230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ensure_directories()\n",
    "\n",
    "base_url = \"https://www.uni-bamberg.de/en/its/\"\n",
    "# Set max_pages to 0 to scrape the whole website, or any positive integer to limit the pages.\n",
    "max_pages = 0\n",
    "\n",
    "visited_file = \"input/explore/visited_urls.txt\"\n",
    "pending_file = \"input/explore/pending_urls.txt\"\n",
    "\n",
    "total_scraped = scrape_website(base_url, max_pages, visited_file, pending_file)\n",
    "print(f\"Scraping completed. Total pages scraped: {total_scraped}.\")"
   ],
   "id": "f3b587f6eb49df3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: 0 https://www.uni-bamberg.de/en/its/\n",
      "Scraping: 1 https://www.uni-bamberg.de/en/its/it-services/\n",
      "Scraping: 2 https://www.uni-bamberg.de/en/its/it-services/login-roles-rights-iam/\n",
      "Scraping: 3 https://www.uni-bamberg.de/en/its/dienstleistungen/support/\n",
      "Scraping: 4 https://www.uni-bamberg.de/en/its/dienstleistungen/support/first-year-first-aid/\n",
      "Scraping: 5 https://www.uni-bamberg.de/en/its/dienstleistungen/support/setting-up-your-work-place-for-home-office/\n",
      "Scraping: 6 https://www.uni-bamberg.de/en/its/dienstleistungen/pc-pools/\n",
      "Scraping: 7 https://www.uni-bamberg.de/en/its/it-services/data-network-wireless-internet-vpn/\n",
      "Scraping: 8 https://www.uni-bamberg.de/en/its/dienstleistungen/eva/\n",
      "Scraping: 9 https://www.uni-bamberg.de/en/its/dienstleistungen/pc/\n",
      "Scraping: 10 https://www.uni-bamberg.de/en/its/dienstleistungen/kurse/\n",
      "Scraping: 11 https://www.uni-bamberg.de/en/its/it-services/webdienste/\n",
      "Scraping: 12 https://www.uni-bamberg.de/en/its/wir/\n",
      "Scraping: 13 https://www.uni-bamberg.de/en/its/dienstleistungen/support/#c201608\n",
      "Scraping: 14 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/user-account-for-students/\n",
      "Scraping: 15 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/\n",
      "Scraping: 16 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/vpn/set-up-vpn/\n",
      "Scraping: 17 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/user-account-for-employees/\n",
      "Scraping: 18 https://www.uni-bamberg.de/en/its/contact-navigation/contact/\n",
      "Scraping: 19 https://www.uni-bamberg.de/en/its/contact-navigation/legal/\n",
      "Scraping: 20 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/user-accounts-for-external-users/\n",
      "Scraping: 21 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/password/\n",
      "Scraping: 22 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/end-of-relation/\n",
      "Scraping: 23 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/translate-to-english-windows-easyroam/\n",
      "Scraping: 24 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/translate-to-english-macos-easyroam/\n",
      "Scraping: 25 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/linux-easyroam/\n",
      "Scraping: 26 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/ios-easyroam/\n",
      "Scraping: 27 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/android-easyroam/\n",
      "Scraping: 28 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/wlan/wlan-verbindung-an-der-uni-bamberg-einrichten/translate-to-english-android-easyroam-alternative/\n",
      "Scraping: 29 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/vpn/set-up-vpn/eduvpn/\n",
      "Scraping: 30 https://www.uni-bamberg.de/en/its/dienstleistungen/netz/vpn/set-up-vpn/eduvpn-1/\n",
      "Scraping: 31 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/user-account-for-doctoral-candidates/\n",
      "Scraping: 32 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/password/set-password/\n",
      "Scraping: 33 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/password/change-password/\n",
      "Scraping: 34 https://www.uni-bamberg.de/en/its/it-services/iam/user-account/password/forgot-password/\n",
      "Scraping completed. Total pages scraped: 35.\n"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
